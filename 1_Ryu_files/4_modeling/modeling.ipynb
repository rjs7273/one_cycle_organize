{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그대로 Colab에 옮기고, 적절한 경로에 파일 업로드하기\n",
    "# TPU v2-8이 더 효율이 좋음\n",
    "# AWS로 사용시 GPU를 써야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yRH9Qb-1Gdj"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeh-2vu61Mgx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_path = \"/content/drive/MyDrive\"\n",
    "for path, dirs, files in os.walk(root_path):\n",
    "    for file in files:\n",
    "\n",
    "        print(os.path.join(path, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NakybyZACFG"
   },
   "source": [
    "### T4 GPU 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBpak3yh3Ggz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample_with_label_1_to_10.csv, sampling_10000.csv 이용해\n",
    "KcBERT 학습시키는 코드\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from tqdm import tqdm  # tqdm 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ------------------------\n",
    "# 1. 데이터셋 정의\n",
    "# ------------------------\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# ------------------------\n",
    "# 2. 모델 정의\n",
    "# ------------------------\n",
    "class KcBERTSentiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, 3)  # [fear, neutral, greed]\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        pooled = outputs[:, 0]  # CLS 토큰\n",
    "        logits = self.linear(self.dropout(pooled))\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# ------------------------\n",
    "# 3. Threshold 스케줄\n",
    "# ------------------------\n",
    "def get_threshold(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0\n",
    "    elif epoch < 4:\n",
    "        return 0.9\n",
    "    elif epoch < 6:\n",
    "        return 0.7\n",
    "    elif epoch < 8:\n",
    "        return 0.6\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "# ------------------------\n",
    "# 4. 학습 루프\n",
    "# ------------------------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    # tqdm으로 진행률 바 추가\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", ncols=100, leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ------------------------\n",
    "# 5. 가장 최근 에포크 로드\n",
    "# ------------------------\n",
    "\n",
    "model_dir = \"/content/drive/MyDrive/KcBERT/model\"\n",
    "pattern = re.compile(r\"kcbert_epoch(\\d+)\\.pt\")\n",
    "\n",
    "def find_latest_model(model_dir):\n",
    "    latest_epoch = -1\n",
    "    latest_path = None\n",
    "    for fname in os.listdir(model_dir):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            if epoch > latest_epoch:\n",
    "                latest_epoch = epoch\n",
    "                latest_path = os.path.join(model_dir, fname)\n",
    "    return latest_path, latest_epoch\n",
    "\n",
    "# ------------------------\n",
    "# 6. 전체 프로세스\n",
    "# ------------------------\n",
    "def main():\n",
    "    # 데이터 로드\n",
    "    labeled_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/삼성전자_sample_with_label_1_to_10.csv\")\n",
    "    full_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/삼성전자_sampling_10000_data.csv\")\n",
    "\n",
    "    labeled_df = labeled_df.dropna(subset=[\"content\"])\n",
    "    full_df = full_df.dropna(subset=[\"content\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 모델 생성 및 로딩\n",
    "    model = KcBERTSentiment()\n",
    "    latest_model_path, latest_epoch = find_latest_model(model_dir)\n",
    "\n",
    "    if latest_model_path:\n",
    "        model.load_state_dict(torch.load(latest_model_path, map_location=\"cpu\"))\n",
    "        model.eval()\n",
    "        print(f\"✅ 가장 최근 모델 로드 완료: {latest_model_path} (epoch {latest_epoch})\")\n",
    "    else:\n",
    "        print(\"🚀 저장된 모델이 없어 새로 시작합니다.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 초기 라벨 데이터셋\n",
    "    labeled_texts = labeled_df[\"content\"].tolist()\n",
    "    labeled_labels = labeled_df[[\"fear_score\", \"neutral_score\", \"greed_score\"]].values.tolist()\n",
    "    labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # 에포크 범위 수정: 최신 에포크부터 학습 시작\n",
    "    for epoch in range(latest_epoch, 10):  # 최신 에포크부터 시작\n",
    "        print(f\"\\n🌀 Epoch {epoch+1}\")\n",
    "        loss = train(model, labeled_loader, optimizer, device)\n",
    "        print(f\"Train loss: {loss:.4f}\")\n",
    "\n",
    "        # 모델 저장\n",
    "        save_path = f\"/content/drive/MyDrive/KcBERT/model/kcbert_epoch{epoch+1}.pt\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"📦 모델 저장 완료: {save_path}\")\n",
    "\n",
    "        # ---------- Pseudo-labeling ----------\n",
    "        threshold = get_threshold(epoch)\n",
    "        print(f\"Using threshold: {threshold}\")\n",
    "        model.eval()\n",
    "\n",
    "        full_texts = full_df[\"content\"].tolist()\n",
    "        full_dataset = SentimentDataset(full_texts, tokenizer=tokenizer)\n",
    "        full_loader = DataLoader(full_dataset, batch_size=64)\n",
    "\n",
    "        pseudo_texts, pseudo_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in full_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask)  # (batch_size, 3)\n",
    "                confidences, _ = torch.max(outputs, dim=1)\n",
    "                mask = confidences > threshold\n",
    "                for i in range(len(mask)):\n",
    "                    if mask[i]:\n",
    "                        pseudo_texts.append(tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True))\n",
    "                        pseudo_labels.append(outputs[i].cpu().numpy())\n",
    "\n",
    "        # 새롭게 확보된 pseudo-labeled 데이터셋 생성\n",
    "        if pseudo_labels:\n",
    "            print(f\"  ✅ Accepted pseudo-labels: {len(pseudo_labels)}\")\n",
    "            labeled_texts += pseudo_texts\n",
    "            labeled_labels += pseudo_labels\n",
    "            labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=256, shuffle=True)\n",
    "        else:\n",
    "            print(\"  ⚠️ No pseudo-labels accepted this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YMQ3TI33odA"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/drive/MyDrive/KcBERT/model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC8KONLs_-Cw"
   },
   "source": [
    "### TPU v2-8 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A-2lPdlBiCH"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample_with_label_1_to_10.csv, sampling_10000.csv 이용해\n",
    "KcBERT 학습시키는 코드\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "sample_with_label_1_to_10.csv, sampling_10000.csv 이용해\n",
    "KcBERT 학습시키는 코드\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ------------------------\n",
    "# 1. 데이터셋 정의\n",
    "# ------------------------\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# ------------------------\n",
    "# 2. 모델 정의\n",
    "# ------------------------\n",
    "class KcBERTSentiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        pooled = outputs[:, 0]\n",
    "        logits = self.linear(self.dropout(pooled))\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# ------------------------\n",
    "# 3. Threshold 스케줄\n",
    "# ------------------------\n",
    "def get_threshold(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0\n",
    "    elif epoch < 4:\n",
    "        return 0.9\n",
    "    elif epoch < 6:\n",
    "        return 0.7\n",
    "    elif epoch < 8:\n",
    "        return 0.6\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "# ------------------------\n",
    "# 4. 학습 루프 (TPU 최적화)\n",
    "# ------------------------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "\n",
    "    parallel_loader = pl.ParallelLoader(dataloader, [device])\n",
    "    tpu_loader = parallel_loader.per_device_loader(device)\n",
    "\n",
    "    for batch in tqdm(tpu_loader, desc=\"Training\", ncols=100, leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        xm.mark_step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(tpu_loader)\n",
    "\n",
    "# ------------------------\n",
    "# 5. 가장 최근 에포크 로드\n",
    "# ------------------------\n",
    "model_dir = \"/content/drive/MyDrive/KcBERT/model\"\n",
    "pattern = re.compile(r\"kcbert_epoch(\\d+)\\.pt\")\n",
    "\n",
    "def find_latest_model(model_dir):\n",
    "    latest_epoch = -1\n",
    "    latest_path = None\n",
    "    for fname in os.listdir(model_dir):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            if epoch > latest_epoch:\n",
    "                latest_epoch = epoch\n",
    "                latest_path = os.path.join(model_dir, fname)\n",
    "    return latest_path, latest_epoch\n",
    "\n",
    "# ------------------------\n",
    "# 6. 전체 프로세스\n",
    "# ------------------------\n",
    "def main():\n",
    "    device = xm.xla_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "\n",
    "    # 데이터 로드\n",
    "    full_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/삼성전자_sampling_10000.csv\")\n",
    "    labeled_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/삼성전자_sample_with_label_1_to_10.csv\")\n",
    "\n",
    "    full_df = full_df.dropna(subset=[\"content\"])\n",
    "    labeled_df = labeled_df.dropna(subset=[\"content\"])\n",
    "\n",
    "    labeled_texts = labeled_df[\"content\"].tolist()\n",
    "    labeled_labels = labeled_df[[\"fear_score\", \"neutral_score\", \"greed_score\"]].values.tolist()\n",
    "\n",
    "    labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 모델 생성 및 로딩\n",
    "    model = KcBERTSentiment()\n",
    "    latest_model_path, latest_epoch = find_latest_model(model_dir)\n",
    "\n",
    "    if latest_model_path:\n",
    "        model.load_state_dict(torch.load(latest_model_path, map_location=\"cpu\"))\n",
    "        model.eval()\n",
    "        print(f\"✅ 가장 최근 모델 로드 완료: {latest_model_path} (epoch {latest_epoch})\")\n",
    "    else:\n",
    "        latest_epoch = 0\n",
    "        print(\"🚀 저장된 모델이 없어 새로 시작합니다.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # 학습 반복\n",
    "    for epoch in range(latest_epoch, 10):\n",
    "        print(f\"\\n🌀 Epoch {epoch+1}\")\n",
    "        loss = train(model, labeled_loader, optimizer, device)\n",
    "        print(f\"Train loss: {loss:.4f}\")\n",
    "\n",
    "        save_path = f\"{model_dir}/kcbert_epoch{epoch+1}.pt\"\n",
    "        xm.save(model.state_dict(), save_path)\n",
    "        print(f\"📦 모델 저장 완료: {save_path}\")\n",
    "\n",
    "        # ---------- Pseudo-labeling ----------\n",
    "        threshold = get_threshold(epoch)\n",
    "        print(f\"Using threshold: {threshold}\")\n",
    "        model.eval()\n",
    "\n",
    "        full_texts = full_df[\"content\"].tolist()\n",
    "        full_dataset = SentimentDataset(full_texts, tokenizer=tokenizer)\n",
    "        full_loader = DataLoader(full_dataset, batch_size=64)\n",
    "\n",
    "        pseudo_texts, pseudo_labels = [], []\n",
    "\n",
    "        parallel_loader = pl.ParallelLoader(full_loader, [device])\n",
    "        tpu_loader = parallel_loader.per_device_loader(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(tpu_loader, desc=\"Pseudo-labeling\", ncols=100, leave=False):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                confidences, _ = torch.max(outputs, dim=1)\n",
    "                mask = confidences > threshold\n",
    "                for i in range(len(mask)):\n",
    "                    if mask[i]:\n",
    "                        pseudo_texts.append(tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True))\n",
    "                        pseudo_labels.append(outputs[i].cpu().numpy())\n",
    "\n",
    "        if pseudo_labels:\n",
    "            print(f\"  ✅ Accepted pseudo-labels: {len(pseudo_labels)}\")\n",
    "            labeled_texts += pseudo_texts\n",
    "            labeled_labels += pseudo_labels\n",
    "            labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "        else:\n",
    "            print(\"  ⚠️ No pseudo-labels accepted this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW59nHgd-ytl"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQHY-9Q_-GIc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "# ------------------------\n",
    "# 1. 모델 예측 함수\n",
    "# ------------------------\n",
    "\n",
    "def predict(model, test_texts, tokenizer, device, max_len=128):\n",
    "    model.eval()  # 평가 모드\n",
    "    inputs = tokenizer(test_texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "\n",
    "    # 텍스트 데이터를 GPU/TPU에 맞게 전달\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    # 결과 출력 (공포, 중립, 탐욕)\n",
    "    predictions = outputs.cpu().numpy()  # CPU로 결과 이동\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. 테스트 예측 함수\n",
    "# ------------------------\n",
    "\n",
    "def test(model, tokenizer, device, test_file_path, output_file_path, max_len=128):\n",
    "    # 테스트 데이터 로드\n",
    "    test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "    # None 값을 빈 문자열로 대체\n",
    "    test_df[\"content\"] = test_df[\"content\"].fillna(\"\")\n",
    "\n",
    "    test_texts = test_df[\"content\"].tolist()\n",
    "\n",
    "    # 예측 수행\n",
    "    predictions = predict(model, test_texts, tokenizer, device, max_len)\n",
    "\n",
    "    # 예측 결과를 DataFrame으로 변환\n",
    "    result_df = pd.DataFrame(predictions, columns=[\"fear_score\", \"neutral_score\", \"greed_score\"])\n",
    "\n",
    "    # 원본에서 필요한 컬럼들 추가\n",
    "    result_df[\"content\"] = test_df[\"content\"]\n",
    "    result_df[\"timestamp\"] = test_df[\"timestamp\"]\n",
    "    result_df[\"delta_seconds\"] = test_df[\"delta_seconds\"]\n",
    "    result_df[\"weight\"] = test_df[\"weight\"]\n",
    "\n",
    "    # 결과 미리보기\n",
    "    print(result_df.head())\n",
    "\n",
    "    # 저장\n",
    "    result_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"✅ 예측 결과 저장 완료: {output_file_path}\")\n",
    "\n",
    "# 모델 로딩\n",
    "device = xm.xla_device()  # TPU 장치 사용\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "\n",
    "# 모델 불러오기 (예시: 이미 학습된 모델 경로에서 로드)\n",
    "model = KcBERTSentiment()\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/KcBERT/model/kcbert_epoch10.pt\", map_location=\"cpu\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# 테스트 예측 실행\n",
    "test_file_path = \"/content/drive/MyDrive/KcBERT/data/삼성전자_testing_5.csv\"\n",
    "output_file_path = \"/content/drive/MyDrive/KcBERT/data/삼성전자_testing_with_labels_5.csv\"\n",
    "test(model, tokenizer, device, test_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVh-ERXP-l90"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "8NakybyZACFG"
   ],
   "gpuType": "V28",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
