{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ëŒ€ë¡œ Colabì— ì˜®ê¸°ê³ , ì ì ˆí•œ ê²½ë¡œì— íŒŒì¼ ì—…ë¡œë“œí•˜ê¸°\n",
    "# TPU v2-8ì´ ë” íš¨ìœ¨ì´ ì¢‹ìŒ\n",
    "# AWSë¡œ ì‚¬ìš©ì‹œ GPUë¥¼ ì¨ì•¼í• ë“¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yRH9Qb-1Gdj"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeh-2vu61Mgx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_path = \"/content/drive/MyDrive\"\n",
    "for path, dirs, files in os.walk(root_path):\n",
    "    for file in files:\n",
    "\n",
    "        print(os.path.join(path, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NakybyZACFG"
   },
   "source": [
    "### T4 GPU ë²„ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBpak3yh3Ggz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample_with_label_1_to_10.csv, sampling_10000.csv ì´ìš©í•´\n",
    "KcBERT í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œ\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from tqdm import tqdm  # tqdm ì„í¬íŠ¸\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ------------------------\n",
    "# 1. ë°ì´í„°ì…‹ ì •ì˜\n",
    "# ------------------------\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# ------------------------\n",
    "# 2. ëª¨ë¸ ì •ì˜\n",
    "# ------------------------\n",
    "class KcBERTSentiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, 3)  # [fear, neutral, greed]\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        pooled = outputs[:, 0]  # CLS í† í°\n",
    "        logits = self.linear(self.dropout(pooled))\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# ------------------------\n",
    "# 3. Threshold ìŠ¤ì¼€ì¤„\n",
    "# ------------------------\n",
    "def get_threshold(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0\n",
    "    elif epoch < 4:\n",
    "        return 0.9\n",
    "    elif epoch < 6:\n",
    "        return 0.7\n",
    "    elif epoch < 8:\n",
    "        return 0.6\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "# ------------------------\n",
    "# 4. í•™ìŠµ ë£¨í”„\n",
    "# ------------------------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  ë°” ì¶”ê°€\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", ncols=100, leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ------------------------\n",
    "# 5. ê°€ì¥ ìµœê·¼ ì—í¬í¬ ë¡œë“œ\n",
    "# ------------------------\n",
    "\n",
    "model_dir = \"/content/drive/MyDrive/KcBERT/model\"\n",
    "pattern = re.compile(r\"kcbert_epoch(\\d+)\\.pt\")\n",
    "\n",
    "def find_latest_model(model_dir):\n",
    "    latest_epoch = -1\n",
    "    latest_path = None\n",
    "    for fname in os.listdir(model_dir):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            if epoch > latest_epoch:\n",
    "                latest_epoch = epoch\n",
    "                latest_path = os.path.join(model_dir, fname)\n",
    "    return latest_path, latest_epoch\n",
    "\n",
    "# ------------------------\n",
    "# 6. ì „ì²´ í”„ë¡œì„¸ìŠ¤\n",
    "# ------------------------\n",
    "def main():\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    labeled_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_sample_with_label_1_to_10.csv\")\n",
    "    full_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_sampling_10000_data.csv\")\n",
    "\n",
    "    labeled_df = labeled_df.dropna(subset=[\"content\"])\n",
    "    full_df = full_df.dropna(subset=[\"content\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ëª¨ë¸ ìƒì„± ë° ë¡œë”©\n",
    "    model = KcBERTSentiment()\n",
    "    latest_model_path, latest_epoch = find_latest_model(model_dir)\n",
    "\n",
    "    if latest_model_path:\n",
    "        model.load_state_dict(torch.load(latest_model_path, map_location=\"cpu\"))\n",
    "        model.eval()\n",
    "        print(f\"âœ… ê°€ì¥ ìµœê·¼ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {latest_model_path} (epoch {latest_epoch})\")\n",
    "    else:\n",
    "        print(\"ğŸš€ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # ì´ˆê¸° ë¼ë²¨ ë°ì´í„°ì…‹\n",
    "    labeled_texts = labeled_df[\"content\"].tolist()\n",
    "    labeled_labels = labeled_df[[\"fear_score\", \"neutral_score\", \"greed_score\"]].values.tolist()\n",
    "    labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # ì—í¬í¬ ë²”ìœ„ ìˆ˜ì •: ìµœì‹  ì—í¬í¬ë¶€í„° í•™ìŠµ ì‹œì‘\n",
    "    for epoch in range(latest_epoch, 10):  # ìµœì‹  ì—í¬í¬ë¶€í„° ì‹œì‘\n",
    "        print(f\"\\nğŸŒ€ Epoch {epoch+1}\")\n",
    "        loss = train(model, labeled_loader, optimizer, device)\n",
    "        print(f\"Train loss: {loss:.4f}\")\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        save_path = f\"/content/drive/MyDrive/KcBERT/model/kcbert_epoch{epoch+1}.pt\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"ğŸ“¦ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "        # ---------- Pseudo-labeling ----------\n",
    "        threshold = get_threshold(epoch)\n",
    "        print(f\"Using threshold: {threshold}\")\n",
    "        model.eval()\n",
    "\n",
    "        full_texts = full_df[\"content\"].tolist()\n",
    "        full_dataset = SentimentDataset(full_texts, tokenizer=tokenizer)\n",
    "        full_loader = DataLoader(full_dataset, batch_size=64)\n",
    "\n",
    "        pseudo_texts, pseudo_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in full_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask)  # (batch_size, 3)\n",
    "                confidences, _ = torch.max(outputs, dim=1)\n",
    "                mask = confidences > threshold\n",
    "                for i in range(len(mask)):\n",
    "                    if mask[i]:\n",
    "                        pseudo_texts.append(tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True))\n",
    "                        pseudo_labels.append(outputs[i].cpu().numpy())\n",
    "\n",
    "        # ìƒˆë¡­ê²Œ í™•ë³´ëœ pseudo-labeled ë°ì´í„°ì…‹ ìƒì„±\n",
    "        if pseudo_labels:\n",
    "            print(f\"  âœ… Accepted pseudo-labels: {len(pseudo_labels)}\")\n",
    "            labeled_texts += pseudo_texts\n",
    "            labeled_labels += pseudo_labels\n",
    "            labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=256, shuffle=True)\n",
    "        else:\n",
    "            print(\"  âš ï¸ No pseudo-labels accepted this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YMQ3TI33odA"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /content/drive/MyDrive/KcBERT/model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC8KONLs_-Cw"
   },
   "source": [
    "### TPU v2-8 ë²„ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A-2lPdlBiCH"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample_with_label_1_to_10.csv, sampling_10000.csv ì´ìš©í•´\n",
    "KcBERT í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œ\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "sample_with_label_1_to_10.csv, sampling_10000.csv ì´ìš©í•´\n",
    "KcBERT í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œ\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ------------------------\n",
    "# 1. ë°ì´í„°ì…‹ ì •ì˜\n",
    "# ------------------------\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# ------------------------\n",
    "# 2. ëª¨ë¸ ì •ì˜\n",
    "# ------------------------\n",
    "class KcBERTSentiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        pooled = outputs[:, 0]\n",
    "        logits = self.linear(self.dropout(pooled))\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# ------------------------\n",
    "# 3. Threshold ìŠ¤ì¼€ì¤„\n",
    "# ------------------------\n",
    "def get_threshold(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1.0\n",
    "    elif epoch < 4:\n",
    "        return 0.9\n",
    "    elif epoch < 6:\n",
    "        return 0.7\n",
    "    elif epoch < 8:\n",
    "        return 0.6\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "# ------------------------\n",
    "# 4. í•™ìŠµ ë£¨í”„ (TPU ìµœì í™”)\n",
    "# ------------------------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "\n",
    "    parallel_loader = pl.ParallelLoader(dataloader, [device])\n",
    "    tpu_loader = parallel_loader.per_device_loader(device)\n",
    "\n",
    "    for batch in tqdm(tpu_loader, desc=\"Training\", ncols=100, leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        xm.mark_step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(tpu_loader)\n",
    "\n",
    "# ------------------------\n",
    "# 5. ê°€ì¥ ìµœê·¼ ì—í¬í¬ ë¡œë“œ\n",
    "# ------------------------\n",
    "model_dir = \"/content/drive/MyDrive/KcBERT/model\"\n",
    "pattern = re.compile(r\"kcbert_epoch(\\d+)\\.pt\")\n",
    "\n",
    "def find_latest_model(model_dir):\n",
    "    latest_epoch = -1\n",
    "    latest_path = None\n",
    "    for fname in os.listdir(model_dir):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            epoch = int(match.group(1))\n",
    "            if epoch > latest_epoch:\n",
    "                latest_epoch = epoch\n",
    "                latest_path = os.path.join(model_dir, fname)\n",
    "    return latest_path, latest_epoch\n",
    "\n",
    "# ------------------------\n",
    "# 6. ì „ì²´ í”„ë¡œì„¸ìŠ¤\n",
    "# ------------------------\n",
    "def main():\n",
    "    device = xm.xla_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    full_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_sampling_10000.csv\")\n",
    "    labeled_df = pd.read_csv(\"/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_sample_with_label_1_to_10.csv\")\n",
    "\n",
    "    full_df = full_df.dropna(subset=[\"content\"])\n",
    "    labeled_df = labeled_df.dropna(subset=[\"content\"])\n",
    "\n",
    "    labeled_texts = labeled_df[\"content\"].tolist()\n",
    "    labeled_labels = labeled_df[[\"fear_score\", \"neutral_score\", \"greed_score\"]].values.tolist()\n",
    "\n",
    "    labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "    # ëª¨ë¸ ìƒì„± ë° ë¡œë”©\n",
    "    model = KcBERTSentiment()\n",
    "    latest_model_path, latest_epoch = find_latest_model(model_dir)\n",
    "\n",
    "    if latest_model_path:\n",
    "        model.load_state_dict(torch.load(latest_model_path, map_location=\"cpu\"))\n",
    "        model.eval()\n",
    "        print(f\"âœ… ê°€ì¥ ìµœê·¼ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {latest_model_path} (epoch {latest_epoch})\")\n",
    "    else:\n",
    "        latest_epoch = 0\n",
    "        print(\"ğŸš€ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # í•™ìŠµ ë°˜ë³µ\n",
    "    for epoch in range(latest_epoch, 10):\n",
    "        print(f\"\\nğŸŒ€ Epoch {epoch+1}\")\n",
    "        loss = train(model, labeled_loader, optimizer, device)\n",
    "        print(f\"Train loss: {loss:.4f}\")\n",
    "\n",
    "        save_path = f\"{model_dir}/kcbert_epoch{epoch+1}.pt\"\n",
    "        xm.save(model.state_dict(), save_path)\n",
    "        print(f\"ğŸ“¦ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "        # ---------- Pseudo-labeling ----------\n",
    "        threshold = get_threshold(epoch)\n",
    "        print(f\"Using threshold: {threshold}\")\n",
    "        model.eval()\n",
    "\n",
    "        full_texts = full_df[\"content\"].tolist()\n",
    "        full_dataset = SentimentDataset(full_texts, tokenizer=tokenizer)\n",
    "        full_loader = DataLoader(full_dataset, batch_size=64)\n",
    "\n",
    "        pseudo_texts, pseudo_labels = [], []\n",
    "\n",
    "        parallel_loader = pl.ParallelLoader(full_loader, [device])\n",
    "        tpu_loader = parallel_loader.per_device_loader(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(tpu_loader, desc=\"Pseudo-labeling\", ncols=100, leave=False):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                confidences, _ = torch.max(outputs, dim=1)\n",
    "                mask = confidences > threshold\n",
    "                for i in range(len(mask)):\n",
    "                    if mask[i]:\n",
    "                        pseudo_texts.append(tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True))\n",
    "                        pseudo_labels.append(outputs[i].cpu().numpy())\n",
    "\n",
    "        if pseudo_labels:\n",
    "            print(f\"  âœ… Accepted pseudo-labels: {len(pseudo_labels)}\")\n",
    "            labeled_texts += pseudo_texts\n",
    "            labeled_labels += pseudo_labels\n",
    "            labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "        else:\n",
    "            print(\"  âš ï¸ No pseudo-labels accepted this round.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW59nHgd-ytl"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQHY-9Q_-GIc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "# ------------------------\n",
    "# 1. ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "# ------------------------\n",
    "\n",
    "def predict(model, test_texts, tokenizer, device, max_len=128):\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ\n",
    "    inputs = tokenizer(test_texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ GPU/TPUì— ë§ê²Œ ì „ë‹¬\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥ (ê³µí¬, ì¤‘ë¦½, íƒìš•)\n",
    "    predictions = outputs.cpu().numpy()  # CPUë¡œ ê²°ê³¼ ì´ë™\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "# ------------------------\n",
    "\n",
    "def test(model, tokenizer, device, test_file_path, output_file_path, max_len=128):\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "    # None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´\n",
    "    test_df[\"content\"] = test_df[\"content\"].fillna(\"\")\n",
    "\n",
    "    test_texts = test_df[\"content\"].tolist()\n",
    "\n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    predictions = predict(model, test_texts, tokenizer, device, max_len)\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    result_df = pd.DataFrame(predictions, columns=[\"fear_score\", \"neutral_score\", \"greed_score\"])\n",
    "\n",
    "    # ì›ë³¸ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë“¤ ì¶”ê°€\n",
    "    result_df[\"content\"] = test_df[\"content\"]\n",
    "    result_df[\"timestamp\"] = test_df[\"timestamp\"]\n",
    "    result_df[\"delta_seconds\"] = test_df[\"delta_seconds\"]\n",
    "    result_df[\"weight\"] = test_df[\"weight\"]\n",
    "\n",
    "    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(result_df.head())\n",
    "\n",
    "    # ì €ì¥\n",
    "    result_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file_path}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”©\n",
    "device = xm.xla_device()  # TPU ì¥ì¹˜ ì‚¬ìš©\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆì‹œ: ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œì—ì„œ ë¡œë“œ)\n",
    "model = KcBERTSentiment()\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/KcBERT/model/kcbert_epoch10.pt\", map_location=\"cpu\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹¤í–‰\n",
    "test_file_path = \"/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_testing_5.csv\"\n",
    "output_file_path = \"/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_testing_with_labels_5.csv\"\n",
    "test(model, tokenizer, device, test_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVh-ERXP-l90"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "8NakybyZACFG"
   ],
   "gpuType": "V28",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
